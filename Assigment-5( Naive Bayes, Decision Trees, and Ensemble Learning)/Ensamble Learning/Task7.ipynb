{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a597afd4",
   "metadata": {},
   "source": [
    "Task 7: Conceptual Questions\n",
    "Answer:\n",
    "1. What is the difference between Bagging and Boosting?\n",
    "2. How does Random Forest reduce variance?\n",
    "3. What is the weakness of boosting-based methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42057e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "1. What is the difference between Bagging and Boosting?\n",
    "\n",
    "Bagging (Bootstrap Aggregating):-\n",
    "Builds multiple models independently using random subsets of the training data (with replacement).\n",
    "Averages their predictions (or uses majority voting) to improve stability and reduce variance.\n",
    "Examples: Random Forest, Bagged Decision Trees.\n",
    "\n",
    "Boosting:-\n",
    "Builds models sequentially, where each new model focuses on correcting errors made by the previous ones.\n",
    "Combines them to form a strong learner.\n",
    "Reduces both bias and variance, but more prone to overfitting.\n",
    "Examples: AdaBoost, Gradient Boosting, XGBoost.\n",
    "\n",
    "2. How does Random Forest reduce variance?\n",
    "\n",
    "Random Forest builds multiple decision trees using different random subsets of:-\n",
    "Training data (via bootstrapping).\n",
    "Features (random feature selection at each split).\n",
    "By averaging predictions from many uncorrelated trees, it:\n",
    "Smooths out noise from individual trees.\n",
    "Reduces overfitting.\n",
    "\n",
    "Results in lower variance and more robust predictions compared to a single decision tree.\n",
    "\n",
    "3. What is the weakness of boosting-based methods?\n",
    "\n",
    "Sensitive to Noisy Data and Outliers:-\n",
    "Because boosting tries to correct the errors of previous models, it can over-focus on noisy or mislabelled data, leading to overfitting.\n",
    "\n",
    "Longer Training Time:-\n",
    "Boosting is sequential, so it can be slower to train compared to parallel methods like bagging.\n",
    "\n",
    "Complexity:\n",
    "Boosting models are more complex and harder to interpret than simpler models like decision trees.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
